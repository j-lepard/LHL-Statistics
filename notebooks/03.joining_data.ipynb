{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data from Part 1 with the data from Part 2 to create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.170233Z",
     "start_time": "2023-09-24T15:55:24.069052Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Import .csv from API calls\n",
    "STATIONS= pd.read_csv('../data/city_bikes.csv')\n",
    "YELP = pd.read_csv('../data/Yelp_location_results.csv')\n",
    "FS = pd.read_csv('../data/FS_location_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       name                        station_id  \\\n0              La Taqueria Pinche Taco Shop  7a19c49f486d7c0c02b3685d7b240448   \n1                                      Saku  7a19c49f486d7c0c02b3685d7b240448   \n2                                 Uma Sushi  7a19c49f486d7c0c02b3685d7b240448   \n3                   Hokkaido Ramen Santouka  7a19c49f486d7c0c02b3685d7b240448   \n4      iDen & Quan Ju De Beijing Duck House  7a19c49f486d7c0c02b3685d7b240448   \n...                                     ...                               ...   \n12115                                   A&W  cc25ae4f093b33ba0afd1dbc0dd20324   \n12116           Dairy Queen / Orange Julius  cc25ae4f093b33ba0afd1dbc0dd20324   \n12117                      Freshslice Pizza  cc25ae4f093b33ba0afd1dbc0dd20324   \n12118                            720 Sweets  cc25ae4f093b33ba0afd1dbc0dd20324   \n12119                            Thai Basil  cc25ae4f093b33ba0afd1dbc0dd20324   \n\n       rating price               lat_long    distance source  \n0         4.0    $$  49.262487,-123.114397  169.517456   yelp  \n1         4.5    $$  49.262487,-123.114397  178.845344   yelp  \n2         4.5   $$$  49.262487,-123.114397  152.157897   yelp  \n3         4.0    $$  49.262487,-123.114397  191.044234   yelp  \n4         3.5   NaN  49.262487,-123.114397  263.641336   yelp  \n...       ...   ...                    ...         ...    ...  \n12115     3.5     $  49.265442,-123.187738  321.985495   yelp  \n12116     4.5   NaN  49.265442,-123.187738  987.901958   yelp  \n12117     3.5     $  49.265442,-123.187738  333.363870   yelp  \n12118     3.5     $  49.265442,-123.187738  782.192237   yelp  \n12119     3.5     $  49.265442,-123.187738  765.806809   yelp  \n\n[12120 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>station_id</th>\n      <th>rating</th>\n      <th>price</th>\n      <th>lat_long</th>\n      <th>distance</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>La Taqueria Pinche Taco Shop</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>4.0</td>\n      <td>$$</td>\n      <td>49.262487,-123.114397</td>\n      <td>169.517456</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Saku</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>4.5</td>\n      <td>$$</td>\n      <td>49.262487,-123.114397</td>\n      <td>178.845344</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Uma Sushi</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>4.5</td>\n      <td>$$$</td>\n      <td>49.262487,-123.114397</td>\n      <td>152.157897</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hokkaido Ramen Santouka</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>4.0</td>\n      <td>$$</td>\n      <td>49.262487,-123.114397</td>\n      <td>191.044234</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>iDen &amp; Quan Ju De Beijing Duck House</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>3.5</td>\n      <td>NaN</td>\n      <td>49.262487,-123.114397</td>\n      <td>263.641336</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12115</th>\n      <td>A&amp;W</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>3.5</td>\n      <td>$</td>\n      <td>49.265442,-123.187738</td>\n      <td>321.985495</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>12116</th>\n      <td>Dairy Queen / Orange Julius</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>4.5</td>\n      <td>NaN</td>\n      <td>49.265442,-123.187738</td>\n      <td>987.901958</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>12117</th>\n      <td>Freshslice Pizza</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>3.5</td>\n      <td>$</td>\n      <td>49.265442,-123.187738</td>\n      <td>333.363870</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>12118</th>\n      <td>720 Sweets</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>3.5</td>\n      <td>$</td>\n      <td>49.265442,-123.187738</td>\n      <td>782.192237</td>\n      <td>yelp</td>\n    </tr>\n    <tr>\n      <th>12119</th>\n      <td>Thai Basil</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>3.5</td>\n      <td>$</td>\n      <td>49.265442,-123.187738</td>\n      <td>765.806809</td>\n      <td>yelp</td>\n    </tr>\n  </tbody>\n</table>\n<p>12120 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### EXPLORATION AND TESTING OF PRIMARY KEY BETWEEN TABLES. \n",
    "\n",
    "#Architecture \n",
    "# - STATIONS will be the LEFT TABLE as it is the 'master' list of station id and the lookup key.\n",
    "# - LAT/LONG for the STATION will be the primary key\n",
    "# - JOIN dataframes on LAT/LONG. Anticipate there will be multiple results per station id (1-many)\n",
    "\n",
    "\n",
    "# STATIONS.drop_duplicates(subset='lat_long',keep=\"first\").count()\n",
    "\n",
    "## Add primary key to Yelp based on the Lat/Long of the associated station coords\n",
    "## Convert it to string or it will just ADD them! doh\n",
    "\n",
    "YELP['lat_long']=YELP['latitude'].astype(str) +\",\"+ YELP['longitude'].astype(str)\n",
    "YELP['source']=\"yelp\"\n",
    "YELP_df = YELP[['name','station_id','rating','price','lat_long','distance','source']]\n",
    "YELP_df\n",
    "## Confirm that generated pkey ('lat_long' can be used to join tables. \n",
    "## Postive outcome: use outpt of concatenated field as input to filter the Station list. \n",
    "## Result : success. \n",
    "## Conclusion: Use the generated 'lat_long' as a primary key to join tables. \n",
    "\n",
    "# Sample Lat_long from Yelp='49.262487,-123.114397'\n",
    "# \n",
    "# STATIONS_filter = STATIONS[STATIONS['lat_long']=='49.262487,-123.114397']\n",
    "# STATIONS_filter\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.173843Z",
     "start_time": "2023-09-24T15:55:24.103108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                          name                        station_id  rating  \\\n0       Rogue Kitchen & Wetbar  7a19c49f486d7c0c02b3685d7b240448     8.2   \n1          33 Acres Brewing Co  7a19c49f486d7c0c02b3685d7b240448     9.4   \n2              The Juice Truck  7a19c49f486d7c0c02b3685d7b240448     8.8   \n3             Biercraft Bistro  7a19c49f486d7c0c02b3685d7b240448     8.1   \n4           Faculty Brewing Co  7a19c49f486d7c0c02b3685d7b240448     8.3   \n...                        ...                               ...     ...   \n2259           The Fringe Cafe  cc25ae4f093b33ba0afd1dbc0dd20324     5.8   \n2260          Coppertank Grill  cc25ae4f093b33ba0afd1dbc0dd20324     5.7   \n2261             Good Co. Kits  cc25ae4f093b33ba0afd1dbc0dd20324     NaN   \n2262  Castaway Bar and Kitchen  cc25ae4f093b33ba0afd1dbc0dd20324     NaN   \n2263        Nick's Barber Shop  cc25ae4f093b33ba0afd1dbc0dd20324     NaN   \n\n      price               lat_long  distance source                    fsq_id  \n0       3.0  49.262487,-123.114397       250     FS  4f60fc8be4b0a9d090a1f850  \n1       2.0  49.262487,-123.114397       674     FS  51b201d27dd249ae714ba728  \n2       1.0  49.262487,-123.114397       779     FS  537e3ec8498ec730b9966b97  \n3       3.0  49.262487,-123.114397       744     FS  4cbf89f200d8370403c8445c  \n4       2.0  49.262487,-123.114397       994     FS  55d97956498e2c299b5830c0  \n...     ...                    ...       ...    ...                       ...  \n2259    2.0  49.265442,-123.187738       955     FS  4aa7f754f964a520484e20e3  \n2260    1.0  49.265442,-123.187738       941     FS  4aa6a91df964a520864a20e3  \n2261    1.0  49.265442,-123.187738       791     FS  6457401321f70230178ea1ea  \n2262    2.0  49.265442,-123.187738       809     FS  5d29827cf5c8330023780aff  \n2263    NaN  49.265442,-123.187738       885     FS  4d31ef57b6093704ac58f1df  \n\n[2264 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>station_id</th>\n      <th>rating</th>\n      <th>price</th>\n      <th>lat_long</th>\n      <th>distance</th>\n      <th>source</th>\n      <th>fsq_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Rogue Kitchen &amp; Wetbar</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>8.2</td>\n      <td>3.0</td>\n      <td>49.262487,-123.114397</td>\n      <td>250</td>\n      <td>FS</td>\n      <td>4f60fc8be4b0a9d090a1f850</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33 Acres Brewing Co</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>9.4</td>\n      <td>2.0</td>\n      <td>49.262487,-123.114397</td>\n      <td>674</td>\n      <td>FS</td>\n      <td>51b201d27dd249ae714ba728</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Juice Truck</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>8.8</td>\n      <td>1.0</td>\n      <td>49.262487,-123.114397</td>\n      <td>779</td>\n      <td>FS</td>\n      <td>537e3ec8498ec730b9966b97</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Biercraft Bistro</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>8.1</td>\n      <td>3.0</td>\n      <td>49.262487,-123.114397</td>\n      <td>744</td>\n      <td>FS</td>\n      <td>4cbf89f200d8370403c8445c</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Faculty Brewing Co</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n      <td>8.3</td>\n      <td>2.0</td>\n      <td>49.262487,-123.114397</td>\n      <td>994</td>\n      <td>FS</td>\n      <td>55d97956498e2c299b5830c0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2259</th>\n      <td>The Fringe Cafe</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>5.8</td>\n      <td>2.0</td>\n      <td>49.265442,-123.187738</td>\n      <td>955</td>\n      <td>FS</td>\n      <td>4aa7f754f964a520484e20e3</td>\n    </tr>\n    <tr>\n      <th>2260</th>\n      <td>Coppertank Grill</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>5.7</td>\n      <td>1.0</td>\n      <td>49.265442,-123.187738</td>\n      <td>941</td>\n      <td>FS</td>\n      <td>4aa6a91df964a520864a20e3</td>\n    </tr>\n    <tr>\n      <th>2261</th>\n      <td>Good Co. Kits</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>49.265442,-123.187738</td>\n      <td>791</td>\n      <td>FS</td>\n      <td>6457401321f70230178ea1ea</td>\n    </tr>\n    <tr>\n      <th>2262</th>\n      <td>Castaway Bar and Kitchen</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>49.265442,-123.187738</td>\n      <td>809</td>\n      <td>FS</td>\n      <td>5d29827cf5c8330023780aff</td>\n    </tr>\n    <tr>\n      <th>2263</th>\n      <td>Nick's Barber Shop</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>49.265442,-123.187738</td>\n      <td>885</td>\n      <td>FS</td>\n      <td>4d31ef57b6093704ac58f1df</td>\n    </tr>\n  </tbody>\n</table>\n<p>2264 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CREATE FOREIGN KEYS IN YELP AND FS DATAFRAMES\n",
    "\n",
    "## Eliminate extra fields in STATIONS\n",
    "STATIONS_abr = STATIONS[['lat_long','name','free_bikes']]\n",
    "STATIONS_abr\n",
    "\n",
    "\n",
    "YELP['lat_long']=YELP['latitude'].astype(str) +\",\"+ YELP['longitude'].astype(str)\n",
    "YELP['source']=\"yelp\"\n",
    "YELP_df = YELP[['name','station_id','rating','price','lat_long','distance','source','id']]\n",
    "YELP_df\n",
    "## QA - CHECK - 12,120 records. Lat_long Generated. \n",
    "\n",
    "FS['lat_long']=FS['latitude'].astype(str) +\",\"+ FS['longitude'].astype(str)\n",
    "FS['source']=\"FS\"\n",
    "FS_df = FS[['name','station_id','rating','price','lat_long','distance','source','fsq_id']]\n",
    "FS_df\n",
    "\n",
    "## QA - Observation - FS may have limited the results as there is an curious consistency about the max number of results per STATION\n",
    "## todo: investigate why there might be a max of then bars per stationid in the FS results \n",
    "##FS.groupby('station_id')['name'].count().sort_values(ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.174469Z",
     "start_time": "2023-09-24T15:55:24.131834Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all your results in an SQLite3 database (remember, SQLite stores its databases as files in your local machine - make sure to create your database in your project's data/ directory!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.191093Z",
     "start_time": "2023-09-24T15:55:24.136735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection to sq3 db successful\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "\n",
    "def create_connection(path):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = sqlite3.connect(path)\n",
    "        print(\"connection to sq3 db successful\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' happened\")\n",
    "\n",
    "    return connection\n",
    "\n",
    "## Create a CONNECTION between python and the sqlite file 'sm_app'\n",
    "connection = create_connection('../data/vancouver_bikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12120 entries, 0 to 12119\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   name        12120 non-null  object \n",
      " 1   station_id  12120 non-null  object \n",
      " 2   rating      12120 non-null  float64\n",
      " 3   price       10169 non-null  object \n",
      " 4   lat_long    12120 non-null  object \n",
      " 5   distance    12120 non-null  float64\n",
      " 6   source      12120 non-null  object \n",
      " 7   id          12120 non-null  object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 757.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Create TABLES\n",
    "\n",
    "## Tables: \n",
    "    ## - vancouver_stations\n",
    "    ## - yelp_data\n",
    "\n",
    "## 1. Confirm list of Fields for tables to create fields in DB accordingly. \n",
    "# print(STATIONS.info())\n",
    "print(YELP_df.info())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.191585Z",
     "start_time": "2023-09-24T15:55:24.144083Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "### DEFINE FUNCTION FOR CREATE/DELETE IN DB \n",
    "\n",
    "def execute_CD_qry(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        connection.commit()\n",
    "        print(\"Query executed successfully\")\n",
    "    except Error as e:\n",
    "        print(f\"Computer reported '{e}' error\")\n",
    "        \n",
    "        \n",
    "def execute_station_update_qry(connection, query):\n",
    "    connection = create_connection('../data/vancouver_bikes')\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        for index, row in STATIONS.iterrows():\n",
    "            try:\n",
    "                # Define your INSERT query\n",
    "                query\n",
    "                \n",
    "                # Get the values from the DataFrame row\n",
    "                values = (\n",
    "                    row['empty_slots'],\n",
    "                    row['free_bikes'],\n",
    "                    row['id'],\n",
    "                    row['latitude'],\n",
    "                    row['longitude'],\n",
    "                    row['name'],\n",
    "                    row['timestamp'],\n",
    "                    row['extra_ebikes'],\n",
    "                    row['extra_has_ebikes'],\n",
    "                    row['extra_last_updated'],\n",
    "                    row['lat_long']\n",
    "                    )\n",
    "        \n",
    "                # Execute the INSERT query with the row values\n",
    "                cursor.execute(query, values)\n",
    "            except Error as e:\n",
    "                print(f\"Error inserting row: {e}\")\n",
    "                connection.rollback()  # Rollback the transaction in case of an error\n",
    "        connection.commit()\n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "def execute_yelp_update_qry(connection, query):\n",
    "    connection = create_connection('../data/vancouver_bikes')\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        for index, row in YELP_df.iterrows():\n",
    "            try:\n",
    "                # Define your INSERT query\n",
    "                query\n",
    "                \n",
    "                # Get the values from the DataFrame row\n",
    "                values = (\n",
    "                     row['name'],\n",
    "                    row['station_id'],\n",
    "                    row['rating'],\n",
    "                    row['price'],\n",
    "                    row['lat_long'],\n",
    "                    row['distance'],\n",
    "                    row['source'],\n",
    "                    row['id']\n",
    "                 \n",
    "                    )\n",
    "        \n",
    "                # Execute the INSERT query with the row values\n",
    "                cursor.execute(query, values)\n",
    "            except Error as e:\n",
    "                print(f\"Error inserting row: {e}\")\n",
    "                connection.rollback()  # Rollback the transaction in case of an error\n",
    "        connection.commit()\n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.191663Z",
     "start_time": "2023-09-24T15:55:24.154614Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "### Create TABLE: vancouver_stations\n",
    "\n",
    "''' PANDA DATA TYPE - STATIONS\n",
    " 0   empty_slots         245 non-null    int64  \n",
    " 1   free_bikes          245 non-null    int64  \n",
    " 2   id                  245 non-null    object \n",
    " 3   latitude            245 non-null    float64\n",
    " 4   longitude           245 non-null    float64\n",
    " 5   name                245 non-null    object \n",
    " 6   timestamp           245 non-null    object \n",
    " 7   extra_ebikes        245 non-null    int64  \n",
    " 8   extra_has_ebikes    245 non-null    bool   \n",
    " 9   extra_last_updated  245 non-null    int64  \n",
    " 10  lat_long            245 non-null    object '''\n",
    "\n",
    "'''MAP TO SQLITE TYPE\n",
    "empty_slots\t        INTEGER\n",
    "free_bikes\t        INTEGER\n",
    "id\t                TEXT\n",
    "latitude\t        REAL\n",
    "longitude\t        REAL\n",
    "name\t            TEXT\n",
    "timestamp\t        TEXT (assuming it's a string representation of date-time)\n",
    "extra_ebikes\t    INTEGER\n",
    "extra_has_ebikes\tINTEGER (0 for False, 1 for True)\n",
    "extra_last_updated\tINTEGER\n",
    "lat_long\t        TEXT\n",
    "'''\n",
    "###########################################\n",
    "'''PANDA DATA TYPE - YELP\n",
    "    name        12120 non-null  object \n",
    " 1   station_id  12120 non-null  object \n",
    " 2   rating      12120 non-null  float64\n",
    " 3   price       10169 non-null  object \n",
    " 4   lat_long    12120 non-null  object \n",
    " 5   distance    12120 non-null  float64\n",
    " 6   source      12120 non-null  object \n",
    " 7   id          12120 non-null  object\n",
    " '''\n",
    "'''MAP TO SQLITE TYPE\n",
    "name  TEXT \n",
    "station_id  TEXT \n",
    "rating  float8\n",
    "price  float8 \n",
    "lat_long  TEXT \n",
    "distance  float8\n",
    "source TEXT \n",
    "id TEXT'''\n",
    "\n",
    "#### QUERIES: CREATE (DELETE) DATA ELEMENTS ######\n",
    "\n",
    "create_vancouver_stations_tble = '''\n",
    "    CREATE TABLE IF NOT EXISTS vancouver_stations (\n",
    "            empty_slots INTEGER,\n",
    "            free_bikes INTEGER,\n",
    "            id TEXT,\n",
    "            latitude REAL,\n",
    "            longitude REAL,\n",
    "            name TEXT,\n",
    "            timestamp TEXT ,\n",
    "            extra_ebikes INTEGER,\n",
    "            extra_has_ebikes INTEGER,\n",
    "            extra_last_updated INTEGER,\n",
    "            lat_long TEXT PRIMARY KEY\n",
    "             );'''\n",
    "\n",
    "create_yelp_tble = '''\n",
    "    CREATE TABLE IF NOT EXISTS yelp_data (\n",
    "        name TEXT, \n",
    "        station_id TEXT, \n",
    "        rating float8,\n",
    "        price float8,\n",
    "        lat_long TEXT, \n",
    "        distance float8,\n",
    "        source TEXT, \n",
    "        id TEXT);'''\n",
    "\n",
    "\n",
    "## QUERY - INSERT RECORDS USING DATAFRAME\n",
    "\n",
    "insert_station_records = \"\"\"\n",
    "        INSERT INTO vancouver_stations (\n",
    "            empty_slots,\n",
    "            free_bikes,\n",
    "            id,\n",
    "            latitude,\n",
    "            longitude,\n",
    "            name,\n",
    "            timestamp,\n",
    "            extra_ebikes,\n",
    "            extra_has_ebikes,\n",
    "            extra_last_updated,\n",
    "            lat_long) \n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "\n",
    "insert_yelp_records = \"\"\"\n",
    "        INSERT INTO yelp_data (\n",
    "            name, \n",
    "            station_id, \n",
    "            rating,\n",
    "            price,\n",
    "            lat_long, \n",
    "            distance,\n",
    "            source, \n",
    "            id) \n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.191727Z",
     "start_time": "2023-09-24T15:55:24.158736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully\n",
      "Query executed successfully\n"
     ]
    }
   ],
   "source": [
    "## CALL DATABASE FUNCTIONS: \n",
    "\n",
    "## CREATE TABLES:\n",
    "execute_CD_qry(connection, create_vancouver_stations_tble)\n",
    "execute_CD_qry(connection,create_yelp_tble)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.191818Z",
     "start_time": "2023-09-24T15:55:24.161648Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data before and after the join to validate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.194569Z",
     "start_time": "2023-09-24T15:55:24.164596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection to sq3 db successful\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n",
      "Error inserting row: UNIQUE constraint failed: vancouver_stations.lat_long\n"
     ]
    }
   ],
   "source": [
    "## INSERT STATION DF into SQLITE DB\n",
    "execute_station_update_qry(connection,insert_station_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection to sq3 db successful\n"
     ]
    }
   ],
   "source": [
    "## INSERT YELP DF into SQLITE DB\n",
    "execute_yelp_update_qry(connection, insert_yelp_records)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.493832Z",
     "start_time": "2023-09-24T15:55:24.178649Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "## INSERT STATION DF into SQLITE DB\n",
    "##### SELECT QUERIES\n",
    "from pprint import pprint\n",
    "def execute_read_query(connection, query):\n",
    "    connection = create_connection('../data/vancouver_bikes')\n",
    "    cursor = connection.cursor()\n",
    "    result = None\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        # Fetch column names\n",
    "        columns = [column[0] for column in cursor.description]\n",
    "        \n",
    "        # Fetch all rows\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # Return as a list of dictionaries\n",
    "        result = [dict(zip(columns, row)) for row in rows]\n",
    "        return result\n",
    "    \n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.511432Z",
     "start_time": "2023-09-24T15:55:24.494953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection to sq3 db successful\n",
      "connection to sq3 db successful\n"
     ]
    },
    {
     "data": {
      "text/plain": "       empty_slots  free_bikes                      id   latitude   longitude  \\\n0                5          29  6iOAgzJ0DRZNSKA3FSrrOg  49.262487 -123.114397   \n1                5          29  XAH2HpuUUtu7CUO26pbs4w  49.262487 -123.114397   \n2                5          29  4118Aq9LbkvUr4s719uUkA  49.262487 -123.114397   \n3                5          29  nkDZY5xqihF3XtZMzzfqqg  49.262487 -123.114397   \n4                5          29  Zi8Ywk36Ws_4zjw2gjMMFA  49.262487 -123.114397   \n...            ...         ...                     ...        ...         ...   \n23935           18           1  YE3XyjvQEacYwjyyHzZN7Q  49.265442 -123.187738   \n23936           18           1  1YPW_QrUrH6-x8ToGIajBw  49.265442 -123.187738   \n23937           18           1  aSA-Wku5H1aGHZi444qmIQ  49.265442 -123.187738   \n23938           18           1  9zTAtAMQs6jcab6w9yUvZw  49.265442 -123.187738   \n23939           18           1  CLxk9A2tocQ0cDJW-P1oTw  49.265442 -123.187738   \n\n                                       name                   timestamp  \\\n0              La Taqueria Pinche Taco Shop  2023-09-21 19:03:51.469000   \n1                                      Saku  2023-09-21 19:03:51.469000   \n2                                 Uma Sushi  2023-09-21 19:03:51.469000   \n3                   Hokkaido Ramen Santouka  2023-09-21 19:03:51.469000   \n4      iDen & Quan Ju De Beijing Duck House  2023-09-21 19:03:51.469000   \n...                                     ...                         ...   \n23935                                   A&W  2023-09-21 19:03:51.672000   \n23936           Dairy Queen / Orange Julius  2023-09-21 19:03:51.672000   \n23937                      Freshslice Pizza  2023-09-21 19:03:51.672000   \n23938                            720 Sweets  2023-09-21 19:03:51.672000   \n23939                            Thai Basil  2023-09-21 19:03:51.672000   \n\n       extra_ebikes  extra_has_ebikes  extra_last_updated  \\\n0                 6                 1          1695322925   \n1                 6                 1          1695322925   \n2                 6                 1          1695322925   \n3                 6                 1          1695322925   \n4                 6                 1          1695322925   \n...             ...               ...                 ...   \n23935             0                 1          1695322779   \n23936             0                 1          1695322779   \n23937             0                 1          1695322779   \n23938             0                 1          1695322779   \n23939             0                 1          1695322779   \n\n                    lat_long  rating price    distance source  \\\n0      49.262487,-123.114397     4.0    $$  169.517456   yelp   \n1      49.262487,-123.114397     4.5    $$  178.845344   yelp   \n2      49.262487,-123.114397     4.5   $$$  152.157897   yelp   \n3      49.262487,-123.114397     4.0    $$  191.044234   yelp   \n4      49.262487,-123.114397     3.5  None  263.641336   yelp   \n...                      ...     ...   ...         ...    ...   \n23935  49.265442,-123.187738     3.5     $  321.985495   yelp   \n23936  49.265442,-123.187738     4.5  None  987.901958   yelp   \n23937  49.265442,-123.187738     3.5     $  333.363870   yelp   \n23938  49.265442,-123.187738     3.5     $  782.192237   yelp   \n23939  49.265442,-123.187738     3.5     $  765.806809   yelp   \n\n                             station_id  \n0      7a19c49f486d7c0c02b3685d7b240448  \n1      7a19c49f486d7c0c02b3685d7b240448  \n2      7a19c49f486d7c0c02b3685d7b240448  \n3      7a19c49f486d7c0c02b3685d7b240448  \n4      7a19c49f486d7c0c02b3685d7b240448  \n...                                 ...  \n23935  cc25ae4f093b33ba0afd1dbc0dd20324  \n23936  cc25ae4f093b33ba0afd1dbc0dd20324  \n23937  cc25ae4f093b33ba0afd1dbc0dd20324  \n23938  cc25ae4f093b33ba0afd1dbc0dd20324  \n23939  cc25ae4f093b33ba0afd1dbc0dd20324  \n\n[23940 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>empty_slots</th>\n      <th>free_bikes</th>\n      <th>id</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>name</th>\n      <th>timestamp</th>\n      <th>extra_ebikes</th>\n      <th>extra_has_ebikes</th>\n      <th>extra_last_updated</th>\n      <th>lat_long</th>\n      <th>rating</th>\n      <th>price</th>\n      <th>distance</th>\n      <th>source</th>\n      <th>station_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>29</td>\n      <td>6iOAgzJ0DRZNSKA3FSrrOg</td>\n      <td>49.262487</td>\n      <td>-123.114397</td>\n      <td>La Taqueria Pinche Taco Shop</td>\n      <td>2023-09-21 19:03:51.469000</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1695322925</td>\n      <td>49.262487,-123.114397</td>\n      <td>4.0</td>\n      <td>$$</td>\n      <td>169.517456</td>\n      <td>yelp</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>29</td>\n      <td>XAH2HpuUUtu7CUO26pbs4w</td>\n      <td>49.262487</td>\n      <td>-123.114397</td>\n      <td>Saku</td>\n      <td>2023-09-21 19:03:51.469000</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1695322925</td>\n      <td>49.262487,-123.114397</td>\n      <td>4.5</td>\n      <td>$$</td>\n      <td>178.845344</td>\n      <td>yelp</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>29</td>\n      <td>4118Aq9LbkvUr4s719uUkA</td>\n      <td>49.262487</td>\n      <td>-123.114397</td>\n      <td>Uma Sushi</td>\n      <td>2023-09-21 19:03:51.469000</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1695322925</td>\n      <td>49.262487,-123.114397</td>\n      <td>4.5</td>\n      <td>$$$</td>\n      <td>152.157897</td>\n      <td>yelp</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>29</td>\n      <td>nkDZY5xqihF3XtZMzzfqqg</td>\n      <td>49.262487</td>\n      <td>-123.114397</td>\n      <td>Hokkaido Ramen Santouka</td>\n      <td>2023-09-21 19:03:51.469000</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1695322925</td>\n      <td>49.262487,-123.114397</td>\n      <td>4.0</td>\n      <td>$$</td>\n      <td>191.044234</td>\n      <td>yelp</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>29</td>\n      <td>Zi8Ywk36Ws_4zjw2gjMMFA</td>\n      <td>49.262487</td>\n      <td>-123.114397</td>\n      <td>iDen &amp; Quan Ju De Beijing Duck House</td>\n      <td>2023-09-21 19:03:51.469000</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1695322925</td>\n      <td>49.262487,-123.114397</td>\n      <td>3.5</td>\n      <td>None</td>\n      <td>263.641336</td>\n      <td>yelp</td>\n      <td>7a19c49f486d7c0c02b3685d7b240448</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23935</th>\n      <td>18</td>\n      <td>1</td>\n      <td>YE3XyjvQEacYwjyyHzZN7Q</td>\n      <td>49.265442</td>\n      <td>-123.187738</td>\n      <td>A&amp;W</td>\n      <td>2023-09-21 19:03:51.672000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1695322779</td>\n      <td>49.265442,-123.187738</td>\n      <td>3.5</td>\n      <td>$</td>\n      <td>321.985495</td>\n      <td>yelp</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n    </tr>\n    <tr>\n      <th>23936</th>\n      <td>18</td>\n      <td>1</td>\n      <td>1YPW_QrUrH6-x8ToGIajBw</td>\n      <td>49.265442</td>\n      <td>-123.187738</td>\n      <td>Dairy Queen / Orange Julius</td>\n      <td>2023-09-21 19:03:51.672000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1695322779</td>\n      <td>49.265442,-123.187738</td>\n      <td>4.5</td>\n      <td>None</td>\n      <td>987.901958</td>\n      <td>yelp</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n    </tr>\n    <tr>\n      <th>23937</th>\n      <td>18</td>\n      <td>1</td>\n      <td>aSA-Wku5H1aGHZi444qmIQ</td>\n      <td>49.265442</td>\n      <td>-123.187738</td>\n      <td>Freshslice Pizza</td>\n      <td>2023-09-21 19:03:51.672000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1695322779</td>\n      <td>49.265442,-123.187738</td>\n      <td>3.5</td>\n      <td>$</td>\n      <td>333.363870</td>\n      <td>yelp</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n    </tr>\n    <tr>\n      <th>23938</th>\n      <td>18</td>\n      <td>1</td>\n      <td>9zTAtAMQs6jcab6w9yUvZw</td>\n      <td>49.265442</td>\n      <td>-123.187738</td>\n      <td>720 Sweets</td>\n      <td>2023-09-21 19:03:51.672000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1695322779</td>\n      <td>49.265442,-123.187738</td>\n      <td>3.5</td>\n      <td>$</td>\n      <td>782.192237</td>\n      <td>yelp</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n    </tr>\n    <tr>\n      <th>23939</th>\n      <td>18</td>\n      <td>1</td>\n      <td>CLxk9A2tocQ0cDJW-P1oTw</td>\n      <td>49.265442</td>\n      <td>-123.187738</td>\n      <td>Thai Basil</td>\n      <td>2023-09-21 19:03:51.672000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1695322779</td>\n      <td>49.265442,-123.187738</td>\n      <td>3.5</td>\n      <td>$</td>\n      <td>765.806809</td>\n      <td>yelp</td>\n      <td>cc25ae4f093b33ba0afd1dbc0dd20324</td>\n    </tr>\n  </tbody>\n</table>\n<p>23940 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## DEFINE A 'SELECT ALL' QUERY\n",
    "select_all = '''\n",
    "            SELECT *\n",
    "            FROM vancouver_stations\n",
    "            JOIN main.yelp_data yd ON vancouver_stations.lat_long = yd.lat_long;'''\n",
    "\n",
    "## CALL THE READ QUERY - \n",
    "execute_read_query(connection, select_all)\n",
    "\n",
    "\n",
    "Full_query_df = pd.DataFrame(execute_read_query(connection,select_all))\n",
    "Full_query_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:55:24.681010Z",
     "start_time": "2023-09-24T15:55:24.497558Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SQL to Clean Yelp Table:\n",
    "\n",
    "![SQL](../images/SQL_to_Clean_yelp.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection to sq3 db successful\n"
     ]
    }
   ],
   "source": [
    "### SELECT QUERY for REGRESSION MODEL\n",
    "import pandas as pd\n",
    "select_restaurants_bike_count = '''\n",
    "        SELECT POI_namerating, price_corrected,rating, sum(total_bikes) as Total_bikes_1km\n",
    "        FROM POI\n",
    "        GROUP BY  POI_namerating\n",
    "        ORDER BY sum(total_bikes) DESC\n",
    "        '''\n",
    "## CALL THE READ QUERY - \n",
    "# execute_read_query(connection, select_restaurants_bike_count)\n",
    "Restaurants_bike_count = pd.DataFrame(execute_read_query(connection, select_restaurants_bike_count))\n",
    "Restaurants_bike_count\n",
    "\n",
    "### EXPORT QUERY Result for Regression model\n",
    "\n",
    "Restaurants_bike_count.to_csv('../data/Restrauts_bike_count.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T16:00:07.561474Z",
     "start_time": "2023-09-24T16:00:07.522899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
